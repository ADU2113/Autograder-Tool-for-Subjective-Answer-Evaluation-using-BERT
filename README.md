Certainly! Here’s a formatted version of your README file for GitHub:

# Autograder Tool for Subjective Answer Evaluation using BERT

## Introduction
Subjective questions and answers let students show what they know in their own words, without strict rules or constraints. However, subjective answers are different from objective ones in a few important ways. They tend to be longer, take more time to write, and require teachers to focus and stay objective while grading. Evaluating a subjective answer means the grader has to carefully review every word, and their mental well-being and fatigue greatly affect the final score.

So, it’s more efficient in terms of time and resources to have a system handle the tedious and crucial task of grading subjective answers. The system proposes a new and improved way of evaluating descriptive question answers automatically using the BERT model and natural language processing.

## Dataset
The dataset used for this project can be found [here](https://www.kaggle.com/datasets/vin1234/janatahack-independence-day-2020-ml-hackathon?resource=download).

### Overview of the Dataset
- **Total entries:** 20,972
- **Columns:** 9
- **Text columns:** TITLE, ABSTRACT
- **Category columns:** Computer Science, Physics, Mathematics, Statistics, Quantitative Biology, Quantitative Finance

### Usage
The dataset can be used for training a machine learning model to classify academic papers into one or more of the specified categories based on their title and abstract.

## Reference
For more detailed information, refer to the [IEEE paper](https://ieeexplore.ieee.org/document/9627669).

---

